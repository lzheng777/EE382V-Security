{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKqrJMOKffAw"
   },
   "source": [
    "# Detecting obfuscated bash commands\n",
    "\n",
    "In this Jupyter notebook, we will study a dataset comprised of bash commands and obfuscated versions of them, using machine learning to try to learn how to detect these obfuscated (and therefore likely malicious) commands. The purpose of this exercise is to demonstrate a standard supervised learning pipeline, consisting of feature extraction, creating a train/test split, training a classfier, analyzing the results, and iterating on this process until we obtain satisfactory performance.\n",
    "\n",
    "In order to circumvent simple, signature-based detectors for suspicious commands executed in the shell, attackers may choose to obfuscate their commands using automated methods. Such tools take a normal command as input and generate a version that is functionally the same but looks significantly different, and would therefore escape detection by a signature-based detector. The obfuscation tool we will be using to generate our example dataset is Bashfuscator [1], which works with bash commands.\n",
    "\n",
    "To generate our example dataset, we downloaded a set of normal (i.e., unobfuscated) bash commands from a project called NL2Bash [2]. This data consists of a set of ~12K bash one-liners [3] collected from websites such as StackOverflow. Using Bashfuscator and a subset of these commands, we generated ~1200 obfuscated bash commands.\n",
    "\n",
    "### References\n",
    "[1] https://github.com/Bashfuscator/Bashfuscator\n",
    "\n",
    "[2] https://github.com/TellinaTool/nl2bash\n",
    "\n",
    "[3] https://github.com/TellinaTool/nl2bash/blob/master/data/bash/all.cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TFQANgUyU8RS"
   },
   "source": [
    "## Load data\n",
    "For this exercise, the data collection and pre-processing steps have already been completed as described earlier, and that we only need to load this dataset in order to start using it.\n",
    "\n",
    "Note that since we are trying to detect obfuscated bash commands, we label obfuscated commands as members of the positive class (+1) and normal (i.e., unobfuscated) ones as members of the negative class (-1). This will be significant later on when we analyze the performance of our classifiers by considering their precision, recall, false positives, false negatives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmB2lg-v0Kxg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Load raw text\n",
    "nor_text = list()\n",
    "mal_text = list()\n",
    "\n",
    "# Load unobfuscated commands\n",
    "sample_proportion = 1  # lower to improve class balance\n",
    "my_file = 'data/bash_commands'\n",
    "with open(my_file) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if random.random() < sample_proportion:\n",
    "            cmd = line.rstrip()\n",
    "            nor_text.append(cmd)\n",
    "\n",
    "# Load obfuscated commands\n",
    "my_file = 'data/obs_bash_commands'\n",
    "with open(my_file) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        cmd = line.rstrip()\n",
    "        mal_text.append(cmd)\n",
    "\n",
    "# Count number of normal and malicious commands\n",
    "num_nor = len(nor_text)\n",
    "num_mal = len(mal_text)\n",
    "print('\\nLoaded %s normal commands and %s obfuscated commmands.' % (num_nor, num_mal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlk8Fh3I4r-L"
   },
   "source": [
    "### Examples from dataset\n",
    "Let's first look at some examples of normal and obfuscated bash commands to better understand the task at hand. This step of manually inspecting the data and trying to understand its features and peculiarities is an important and necessary one, as it informs us on the type of model we may want to use to best solve our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8Iao9y6hx8t"
   },
   "outputs": [],
   "source": [
    "# Examples of normal bash commands\n",
    "print('Normal bash examples:')\n",
    "for elem in nor_text[0:10]: print('(len: ' + str(len(elem)) + ') ' + elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpcXVVkjkS9m"
   },
   "outputs": [],
   "source": [
    "# Examples of obfuscated bash commands\n",
    "print('Obfuscated bash examples:')\n",
    "for elem in mal_text[0:10]: print('(len: ' + str(len(elem)) + ') ' + elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I864Nr5e-cs0"
   },
   "source": [
    "### Length of commands\n",
    "We inspect the distribution of lengths of normal and obfuscated commands to determine whether there are any patterns that can be used to discriminate between the two classes.\n",
    "\n",
    "*Bonus: Can you prescribe a hand-coded set of rules (based on command length) that does well at discriminating between normal and obfuscated commands?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjt0JWyfzPqd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute log (base 10) lengths of normal and obfuscated commands\n",
    "len_nor = [len(elem) for elem in nor_text]\n",
    "loglen_nor = np.log10(len_nor)\n",
    "\n",
    "len_mal = [len(elem) for elem in mal_text]\n",
    "loglen_mal = np.log10(len_mal)\n",
    "\n",
    "# Evalute max of log lengths\n",
    "print('Max. log length for normal commands: ' + str(np.max(loglen_nor)))\n",
    "print('Max. log length for obfuscated commands: ' + str(np.max(loglen_mal)))\n",
    "\n",
    "# Plot density\n",
    "bins = np.linspace(0, 8, 160)\n",
    "plt.hist(loglen_nor, bins, density=True, alpha=0.5, label='normal')\n",
    "plt.hist(loglen_mal, bins, density=True, alpha=0.5, label='obfuscated')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Density plot of log (base 10) command lengths')\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = [8,6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntO9FPDV3zhJ"
   },
   "outputs": [],
   "source": [
    "# Truncate obfuscated commands\n",
    "truncate = True\n",
    "hard_truncate = False\n",
    "max_length = 100\n",
    "\n",
    "if truncate:\n",
    "    if hard_truncate:\n",
    "        # Truncate all obfuscated commands at specified max length\n",
    "        mal_text = [elem[:max_length] for elem in mal_text]\n",
    "    else:\n",
    "        # Truncate obfuscated commands according to normal command length distribution\n",
    "        mal_text = [elem[:random.choice(len_nor)] for elem in mal_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWZuvfKIVFJM"
   },
   "source": [
    "## Feature extraction\n",
    "We now perform feature extraction on a labeled dataset comprised of both normal and obfuscated commands. To do this, we will use the `CountVectorizer` function in scikit-learn to generate a set of features based on $n$-grams of words or characters (or some other user-specified criterion), then compute the number of occurrences of every feature within each command. Each such count vector can be thought of a vector representation (e.g., an embedding) of the command from which it was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDEl678t_9M7"
   },
   "outputs": [],
   "source": [
    "# Merge normal/obfuscated commands into one labeled dataset\n",
    "raw_text = nor_text + mal_text\n",
    "raw_labels = [-1]*num_nor + [1]*num_mal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, 1, 1, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[-1]*5 + [1]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlJXAcZE0P0d"
   },
   "outputs": [],
   "source": [
    "# Build feature extractor\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count_vect = CountVectorizer()  # word feature extraction\n",
    "count_vect = CountVectorizer(analyzer='char', ngram_range=(1,2))  # character n-gram feature extraction\n",
    "\n",
    "# Extract feature counts\n",
    "raw_counts = count_vect.fit_transform(raw_text)\n",
    "\n",
    "# Display features\n",
    "features = count_vect.get_feature_names()\n",
    "print('Feature set: ' + str(features))\n",
    "print('Number of features: ' + str(len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcgqjOtGBi7n"
   },
   "outputs": [],
   "source": [
    "# Normalize counts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "all_data = tf_transformer.fit_transform(raw_counts)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "import numpy as np\n",
    "all_labels = np.asarray(raw_labels)\n",
    "\n",
    "# Create set of indices\n",
    "indices = np.arange(len(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0DjVKblaGVh"
   },
   "source": [
    "## Train/test split\n",
    "In order to not overfit the model and miscalculate its true performance, we must first split the dataset into a training set and a test set. In the following sections, we will train a model using feature vectors and corresponding labels from the training set (see **Model training**), and evaluate its performance by predicting labels using only feature vectors from the test set (see **Inference**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eq9eeBhQaJva"
   },
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Include indices for tracking of individual data points after splitting\n",
    "train_data, test_data, train_labels, test_labels, train_indices, test_indices = train_test_split(all_data, all_labels, indices, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItVaVhl8U2PD"
   },
   "source": [
    "## Model training\n",
    "Next, we can train a classifier that tries to learn which parts of the feature space correspond to each of the two classes: normal vs. obfuscated. We will focus on logistic regression (a linear model) and random forest (a non-linear, ensemble method) in order to illustrate how these classifiers can be utilized, and will later show how their results can be analyzed and interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTMGlhQRaO8k"
   },
   "source": [
    "### Random forest (ensemble of decision trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCeBl8LFZzBB"
   },
   "outputs": [],
   "source": [
    "# Train a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=250, max_depth=5, random_state=0).fit(train_data, train_labels)\n",
    "classifier_type = 'random forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CmzUwA2KR3W"
   },
   "outputs": [],
   "source": [
    "# Train a decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(max_depth=5).fit(train_data, train_labels)\n",
    "classifier_type = 'decision tree'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEWlECL4aG38"
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ER8--rFz5rut"
   },
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier using stochastic gradient descent (SGD)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier = SGDClassifier(loss='log', penalty='none', random_state=0).fit(train_data, train_labels)\n",
    "classifier_type = 'logistic regression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAfVCiNqZ32u"
   },
   "source": [
    "## Inference\n",
    "In this section, we apply the trained classifier to the test set and study its performance using a series of metrics. We can also take a closer look at those examples that were misclassified and try to discern the reasons why prediction failed. To do this, we inspect coefficients within the trained model in order gain a better understanding of what it has learned and how it could potentially be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcWq9G644GsN"
   },
   "outputs": [],
   "source": [
    "# Predict labels for test data\n",
    "predicted_labels = classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_1kOICNo3bs"
   },
   "source": [
    "### Analyze performance\n",
    "We consider the performance of our classifier by looking at metrics such as precision, recall, F1-measure, and accuracy. These can also be built using the confusion matrix, which is essentially a histogram of the predicted labels and true lables of examples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gujD1MssveIQ"
   },
   "outputs": [],
   "source": [
    "# Analyze performance\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classification report\n",
    "#print(metrics.classification_report(test_labels, predicted_labels))\n",
    "\n",
    "# Standard metrics\n",
    "precision = metrics.precision_score(test_labels, predicted_labels)\n",
    "recall = metrics.recall_score(test_labels, predicted_labels)\n",
    "f1measure = metrics.f1_score(test_labels, predicted_labels)\n",
    "accuracy = metrics.accuracy_score(test_labels, predicted_labels)\n",
    "\n",
    "print(' precision = ' + str(precision))\n",
    "print('    recall = ' + str(recall))\n",
    "print('F1-measure = ' + str(f1measure))\n",
    "print('  accuracy = ' + str(accuracy))\n",
    "print('\\n')\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion matrix (text-only):')\n",
    "cm = metrics.confusion_matrix(test_labels, predicted_labels, classifier.classes_)\n",
    "print(classifier.classes_)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "SHbJqFL-l1Bn"
   },
   "outputs": [],
   "source": [
    "# Plot fancy confusion matrix\n",
    "# Reference: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.4f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "# Pretty plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(test_labels, predicted_labels, classes=['normal','obfuscated'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Pretty plot normalized confusion matrix\n",
    "plot_confusion_matrix(test_labels, predicted_labels, classes=['normal','obfuscated'], normalize=True,\n",
    "                      title='Normalized confusion matrix, by true label')\n",
    "\n",
    "#from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = [6,6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWxUv9Y2oycX"
   },
   "source": [
    "### Inspect misclassified examples\n",
    "By looking at those examples that were misclassified, we can better understand what the classifier has learned and how it could potentially be improved. In our case, each misclassified example is either (i) a command marked as obfuscated although it was actually normal (i.e., a false positive) or (ii) a command marked as normal although it was actually obfuscated (i.e., a false negative). It is largely domain-dependent to determine how to balance the cost of a false negative versus a false positive, and which we'd prefer our model to try to avoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3rhieJyW9hL"
   },
   "outputs": [],
   "source": [
    "# Show misclassified examples\n",
    "misclassified_fp = np.where((test_labels != predicted_labels) & (predicted_labels == np.ones(len(predicted_labels))))\n",
    "misclassified_fn = np.where((test_labels != predicted_labels) & (predicted_labels != np.ones(len(predicted_labels))))\n",
    "\n",
    "false_positives = [raw_text[index] for index in test_indices[misclassified_fp]]\n",
    "print('False positives (marked as obfuscated, but actually normal):')\n",
    "for elem in false_positives:\n",
    "    print(elem)\n",
    "print('\\n')\n",
    "\n",
    "false_negatives = [raw_text[index] for index in test_indices[misclassified_fn]]\n",
    "print('False negatives (marked as normal, but actually obfuscated):')\n",
    "for elem in false_negatives:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1pOJvL2pMma"
   },
   "source": [
    "### Explain model results by analyzing feature importance\n",
    "Finally, we would like to inspect our trained model more closely to try to determine how it is making its predictions. This largely depends entirely on the model we are training, although there are several model-agnostic methods for interpretability that we won't discuss here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UstCLLlmLL7O"
   },
   "source": [
    "#### Random forest (or decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azhbB69KrBY3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if classifier_type in ['decision tree', 'random forest']:\n",
    "\n",
    "    # Compute feature importances\n",
    "    importance_scores = classifier.feature_importances_\n",
    "    feature_importances = pd.DataFrame(importance_scores, index = features,\n",
    "                                columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "    print('Top features based on (averaged) Gini index:')\n",
    "    print(feature_importances[:20])\n",
    "\n",
    "    # Extract (sample) decision tree\n",
    "    if classifier_type == 'decision tree':\n",
    "        sample_tree = classifier\n",
    "    if classifier_type == 'random forest':\n",
    "        sample_tree = random.choice(classifier.estimators_)\n",
    "\n",
    "    # Plot decision tree\n",
    "    from sklearn.tree import plot_tree\n",
    "    plot_tree(sample_tree, feature_names=features, filled=True)\n",
    "\n",
    "    from matplotlib import rcParams\n",
    "    rcParams['figure.figsize'] = [18,10]\n",
    "    print('\\n')\n",
    "    print('(Sampled) decision tree:')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Invalid: Train tree-based classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7SwmE5FLJGE"
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRe-hJRzKxoU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if classifier_type == 'logistic regression':\n",
    "\n",
    "    # CompUte feature importances\n",
    "    importance_scores = np.transpose(classifier.coef_)\n",
    "    feature_importances = pd.DataFrame(importance_scores, index = features,\n",
    "                                columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Display most important features for each class\n",
    "    print('Features that favor obfuscated class:')\n",
    "    print(feature_importances[:10])\n",
    "    print('\\n')\n",
    "    print('Features that favor normal class:')\n",
    "    print(feature_importances[-10:])\n",
    "\n",
    "else:\n",
    "    print(\"Invalid: Train logistic regression classifier.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pdBB8OdXlFwi",
    "HTMGlhQRaO8k",
    "FFosuvImdrg0",
    "RQAuX8igWOx9",
    "UstCLLlmLL7O",
    "k7SwmE5FLJGE"
   ],
   "name": "classdemo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "596935ea77931a3d3a0f7fec3d718aec427de1a662ceaadd9ccf6b25c30472da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
